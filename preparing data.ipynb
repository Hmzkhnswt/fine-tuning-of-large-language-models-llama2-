{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[42, 717, 5516, 4019, 14355, 15, 309, 717, 432, 13115]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am Hamza Ali. I am from Pakistan\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am Hamza Ali. I am from Pakistan'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text = tokenizer.decode(encoded_text)\n",
    "decode_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I am a Muslim\"\n",
    "sentence2 = \"I pray 5 times a day\"\n",
    "sentence3 = \"I always speaks truth and eat health\"\n",
    "sentence4 = \"I get up early in the morning and sleep to bed at 12PM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [42, 717, 247, 8797], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [42, 12518, 608, 2069, 247, 1388], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [42, 1900, 16544, 5083, 285, 6008, 1786], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [42, 755, 598, 2393, 275, 253, 4131, 285, 4600, 281, 3722, 387, 1249, 9122], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "list1 = [sentence1, sentence2, sentence3, sentence4]\n",
    "for i in range(len(list1)):\n",
    "    encoded_sentences = tokenizer(list1[i], padding=True)\n",
    "    print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [42, 717, 247, 8797], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [42, 12518, 608, 2069, 247], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [42, 1900, 16544, 5083, 285], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [42, 755, 598, 2393, 275], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "list1 = [sentence1, sentence2, sentence3, sentence4]\n",
    "for i in range(len(list1)):\n",
    "    encoded_sentences = tokenizer(list1[i], padding=True, max_length=5, truncation=True)\n",
    "    print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset:\n",
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question:\\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n",
       "          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n",
       "           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n",
       "          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n",
       "         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n",
       "         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n",
       "           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n",
       "        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n",
       "         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n",
       "           15,  7280,    15,   900, 14206]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenxed_1 = tokenizer(finetuning_dataset[0]['question'] + finetuning_dataset[0]['answer'] , return_tensors=\"np\", padding=True)\n",
    "tokenxed_1[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenxed_1[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenxed_1 = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1276   403   253  1027  3510   273  7177  2130   275   253 18491   313\n",
      "     70    15    72   904 12692  7102    13  8990 10097    13 13722   434\n",
      "   7102  6177    45  4988    74   556 10097   327 27669 11075   264    13\n",
      "   5271 23058    13 19782 37741 10031    13 13814 11397    13   378 16464\n",
      "     13 11759 10535  1981    13 21798 12989    13   285   966 10097   327\n",
      "  21708    46 10797  2130   387  5987  1358    77  4988    74    14  2284\n",
      "     15  7280    15   900 14206]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenxed_1[\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
